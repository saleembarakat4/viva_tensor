//// RTX 4090 Optimized Engine
////
//// RTX 4090 ASUS ROG STRIX SPECIFICATIONS:
//// - GPU: AD102 (16384 CUDA Cores)
//// - Tensor Cores: 512 (4th Gen)
//// - VRAM: 24GB GDDR6X
//// - Bandwidth: 1008 GB/s
//// - TDP: 450W (boost up to 600W)
//// - FP32: 82.6 TFLOPS
//// - FP16 Tensor: 330 TFLOPS
//// - INT8 Tensor: 661 TOPS
////
//// SPECIFIC OPTIMIZATIONS:
//// 1. VRAM-aware batch sizing (24GB - 2GB system = 22GB usable)
//// 2. Tensor Core utilization (8x8 or 16x16 alignment)
//// 3. GDDR6X burst patterns (256-bit bus, aligned access)
//// 4. CUDA Warp-aware parallelism (32 threads)
////
//// Pure Gleam + BEAM concurrency for maximum utilization!

import gleam/float
import gleam/int
import gleam/io
import gleam/list
import viva_tensor/core/tensor.{type Tensor}
import viva_tensor/optim/blackwell.{
  type BlackwellTensor, compress, int8_config, nvfp4_config,
}

// ============================================================================
// RTX 4090 SPECS
// ============================================================================

/// RTX 4090 specifications
pub type Rtx4090Specs {
  Rtx4090Specs(
    /// CUDA Cores
    cuda_cores: Int,
    /// Tensor Cores (4th Gen)
    tensor_cores: Int,
    /// VRAM in GB
    vram_gb: Float,
    /// Available VRAM (after system)
    vram_available_gb: Float,
    /// Bandwidth in GB/s
    bandwidth_gbps: Float,
    /// TDP in Watts
    tdp_watts: Int,
    /// TFLOPS FP32
    tflops_fp32: Float,
    /// TFLOPS FP16 (Tensor)
    tflops_fp16: Float,
    /// TOPS INT8 (Tensor)
    tops_int8: Float,
    /// Warp size (threads per warp)
    warp_size: Int,
    /// SM count
    sm_count: Int,
    /// L2 Cache in MB
    l2_cache_mb: Int,
  )
}

/// Returns RTX 4090 specs
pub fn get_specs() -> Rtx4090Specs {
  Rtx4090Specs(
    cuda_cores: 16_384,
    tensor_cores: 512,
    vram_gb: 24.0,
    vram_available_gb: 22.0,
    // 2GB reserved for system
    bandwidth_gbps: 1008.0,
    tdp_watts: 450,
    tflops_fp32: 82.6,
    tflops_fp16: 330.0,
    tops_int8: 661.0,
    warp_size: 32,
    sm_count: 128,
    l2_cache_mb: 72,
  )
}

// ============================================================================
// RTX 4090 SPECIFIC OPTIMIZATIONS
// ============================================================================

/// Optimized configuration for RTX 4090
pub type Rtx4090Config {
  Rtx4090Config(
    /// Optimal batch size for 24GB VRAM
    optimal_batch_size: Int,
    /// Tile size for Tensor Cores (8 or 16)
    tensor_core_tile: Int,
    /// Memory alignment (256 bits = 32 bytes)
    memory_alignment: Int,
    /// Threads per CUDA block
    threads_per_block: Int,
    /// Use Tensor Cores (FP16/INT8)
    use_tensor_cores: Bool,
    /// Quantization mode
    quant_mode: QuantMode4090,
  )
}

/// Quantization modes for RTX 4090
pub type QuantMode4090 {
  /// Pure FP32 (82.6 TFLOPS)
  Fp32Mode
  /// FP16 with Tensor Cores (330 TFLOPS, 4x FP32!)
  Fp16TensorMode
  /// INT8 with Tensor Cores (661 TOPS, 8x FP32!)
  Int8TensorMode
  /// Mixed precision (FP16 compute, FP32 accumulate)
  MixedPrecisionMode
}

/// Default optimized configuration
pub fn default_config() -> Rtx4090Config {
  let _specs = get_specs()

  // Compute optimal batch size
  // Assuming typical model: 512 dims, ~2MB per batch
  // 22GB available / 2MB = ~11000 simultaneous batches
  // But Tensor Cores work better with batches of 64-256
  let batch_size = 128
  // Sweet spot for Tensor Cores

  Rtx4090Config(
    optimal_batch_size: batch_size,
    tensor_core_tile: 16,
    // 16x16 tiles for maximum efficiency
    memory_alignment: 32,
    // 256 bits = 32 bytes
    threads_per_block: 256,
    // 8 warps per block
    use_tensor_cores: True,
    quant_mode: Int8TensorMode,
    // 661 TOPS!
  )
}

/// Configuration for maximum precision
pub fn precision_config() -> Rtx4090Config {
  Rtx4090Config(
    ..default_config(),
    use_tensor_cores: False,
    quant_mode: Fp32Mode,
  )
}

/// Configuration for maximum speed
pub fn speed_config() -> Rtx4090Config {
  Rtx4090Config(
    ..default_config(),
    optimal_batch_size: 256,
    // Larger batch
    use_tensor_cores: True,
    quant_mode: Int8TensorMode,
  )
}

// ============================================================================
// MEMORY MANAGEMENT
// ============================================================================

/// GPU memory state
pub type GpuMemoryState {
  GpuMemoryState(
    /// Total VRAM in bytes
    total_bytes: Int,
    /// Used VRAM in bytes
    used_bytes: Int,
    /// Free VRAM in bytes
    free_bytes: Int,
    /// Allocated tensors
    allocated_tensors: Int,
    /// Cached bytes
    cached_bytes: Int,
  )
}

/// Creates initial memory state for RTX 4090
pub fn init_memory() -> GpuMemoryState {
  let specs = get_specs()
  let total = float.round(specs.vram_available_gb *. 1024.0 *. 1024.0 *. 1024.0)

  GpuMemoryState(
    total_bytes: total,
    used_bytes: 0,
    free_bytes: total,
    allocated_tensors: 0,
    cached_bytes: 0,
  )
}

/// Computes memory required for tensor
pub fn tensor_memory_bytes(shape: List(Int), mode: QuantMode4090) -> Int {
  let elements = list.fold(shape, 1, fn(acc, d) { acc * d })

  let bytes_per_element = case mode {
    Fp32Mode -> 4
    Fp16TensorMode -> 2
    Int8TensorMode -> 1
    MixedPrecisionMode -> 2
  }

  elements * bytes_per_element
}

/// Checks if tensor fits in VRAM
pub fn can_allocate(state: GpuMemoryState, bytes: Int) -> Bool {
  state.free_bytes >= bytes
}

/// Allocates memory for tensor
pub fn allocate(
  state: GpuMemoryState,
  bytes: Int,
) -> Result(GpuMemoryState, String) {
  case can_allocate(state, bytes) {
    True -> {
      Ok(
        GpuMemoryState(
          ..state,
          used_bytes: state.used_bytes + bytes,
          free_bytes: state.free_bytes - bytes,
          allocated_tensors: state.allocated_tensors + 1,
        ),
      )
    }
    False -> {
      Error(
        "OOM: Not enough VRAM. Free: "
        <> int.to_string(state.free_bytes / 1024 / 1024)
        <> "MB, "
        <> "Required: "
        <> int.to_string(bytes / 1024 / 1024)
        <> "MB",
      )
    }
  }
}

/// Frees memory
pub fn free(state: GpuMemoryState, bytes: Int) -> GpuMemoryState {
  GpuMemoryState(
    ..state,
    used_bytes: int.max(0, state.used_bytes - bytes),
    free_bytes: int.min(state.total_bytes, state.free_bytes + bytes),
    allocated_tensors: int.max(0, state.allocated_tensors - 1),
  )
}

// ============================================================================
// OPTIMIZED BATCH PROCESSING
// ============================================================================

/// Batch processing result
pub type BatchResult {
  BatchResult(
    tensors: List(BlackwellTensor),
    total_time_ms: Int,
    throughput_tps: Float,
    compression_ratio: Float,
    memory_saved_mb: Float,
  )
}

/// Processes batch of tensors with compression
pub fn process_batch(
  tensors: List(Tensor),
  config: Rtx4090Config,
) -> BatchResult {
  let quant_config = case config.quant_mode {
    Int8TensorMode -> int8_config()
    _ -> nvfp4_config()
  }

  // Process in parallel using BEAM
  let parent = erlang_self()
  let indexed = list.index_map(tensors, fn(t, i) { #(i, t) })

  // Spawn workers
  list.each(indexed, fn(pair) {
    let #(idx, t) = pair
    erlang_spawn(fn() {
      let compressed = compress(t, quant_config)
      erlang_send(parent, #(idx, compressed))
    })
  })

  // Timer
  let start = erlang_monotonic_time()

  // Collect results
  let results =
    collect_n(list.length(tensors))
    |> list.sort(fn(a, b) {
      let #(i1, _) = a
      let #(i2, _) = b
      int.compare(i1, i2)
    })
    |> list.map(fn(pair) {
      let #(_, t) = pair
      t
    })

  let end = erlang_monotonic_time()
  let time_ns = end - start
  let time_ms = time_ns / 1_000_000

  // Statistics
  let total_original =
    list.fold(tensors, 0, fn(acc, t) {
      acc + { list.length(tensor.to_list(t)) * 4 }
    })

  let total_compressed =
    list.fold(results, 0, fn(acc, bt) { acc + bt.memory_bytes })

  let ratio = int.to_float(total_original) /. int.to_float(total_compressed)
  let saved_mb =
    int.to_float(total_original - total_compressed) /. 1024.0 /. 1024.0
  let throughput =
    int.to_float(list.length(tensors)) /. { int.to_float(time_ms) /. 1000.0 }

  BatchResult(
    tensors: results,
    total_time_ms: time_ms,
    throughput_tps: throughput,
    compression_ratio: ratio,
    memory_saved_mb: saved_mb,
  )
}

// ============================================================================
// PERFORMANCE ESTIMATOR
// ============================================================================

/// Performance estimate
pub type PerformanceEstimate {
  PerformanceEstimate(
    /// Theoretical FLOPS
    theoretical_flops: Float,
    /// Achievable FLOPS (with overhead)
    achievable_flops: Float,
    /// Estimated time in ms
    estimated_time_ms: Float,
    /// Bottleneck (compute or memory)
    bottleneck: Bottleneck,
    /// Estimated efficiency
    efficiency_pct: Float,
  )
}

/// Bottleneck type
pub type Bottleneck {
  ComputeBound
  MemoryBound
  LatencyBound
}

/// Estimates performance for tensor operation
pub fn estimate_performance(
  flops_needed: Float,
  bytes_to_transfer: Float,
  config: Rtx4090Config,
) -> PerformanceEstimate {
  let specs = get_specs()

  // Available TFLOPS based on mode
  let available_tflops = case config.quant_mode {
    Fp32Mode -> specs.tflops_fp32
    Fp16TensorMode -> specs.tflops_fp16
    Int8TensorMode -> specs.tops_int8
    MixedPrecisionMode -> specs.tflops_fp16
  }

  // Time for compute (in seconds)
  let compute_time = flops_needed /. { available_tflops *. 1.0e12 }

  // Time for memory transfer (in seconds)
  let memory_time = bytes_to_transfer /. { specs.bandwidth_gbps *. 1.0e9 }

  // Bottleneck
  let bottleneck = case compute_time >. memory_time {
    True -> ComputeBound
    False -> MemoryBound
  }

  // Total time (assuming partial overlap)
  let total_time = float.max(compute_time, memory_time) *. 1.2
  // 20% overhead

  // Efficiency
  let theoretical_time = float.max(compute_time, memory_time)
  let efficiency = { theoretical_time /. total_time } *. 100.0

  PerformanceEstimate(
    theoretical_flops: available_tflops *. 1.0e12,
    achievable_flops: available_tflops *. 1.0e12 *. { efficiency /. 100.0 },
    estimated_time_ms: total_time *. 1000.0,
    bottleneck: bottleneck,
    efficiency_pct: efficiency,
  )
}

// ============================================================================
// BENCHMARK
// ============================================================================

pub fn main() {
  benchmark_rtx4090()
}

pub fn benchmark_rtx4090() {
  let specs = get_specs()

  io.println(
    "╔══════════════════════════════════════════════════════════════════╗",
  )
  io.println(
    "║  RTX 4090 ASUS ROG STRIX - OPTIMIZED ENGINE                      ║",
  )
  io.println(
    "║  Pure Gleam maximizing NVIDIA hardware!                          ║",
  )
  io.println(
    "╚══════════════════════════════════════════════════════════════════╝\n",
  )

  io.println("RTX 4090 SPECIFICATIONS:")
  io.println("  CUDA Cores:    " <> int.to_string(specs.cuda_cores))
  io.println(
    "  Tensor Cores:  " <> int.to_string(specs.tensor_cores) <> " (4th Gen)",
  )
  io.println(
    "  VRAM:          " <> float_to_string(specs.vram_gb) <> " GB GDDR6X",
  )
  io.println(
    "  Bandwidth:     " <> float_to_string(specs.bandwidth_gbps) <> " GB/s",
  )
  io.println("  L2 Cache:      " <> int.to_string(specs.l2_cache_mb) <> " MB")
  io.println("")
  io.println(
    "  FP32:          " <> float_to_string(specs.tflops_fp32) <> " TFLOPS",
  )
  io.println(
    "  FP16 Tensor:   "
    <> float_to_string(specs.tflops_fp16)
    <> " TFLOPS (4x FP32!)",
  )
  io.println(
    "  INT8 Tensor:   "
    <> float_to_string(specs.tops_int8)
    <> " TOPS (8x FP32!)",
  )

  // Memory state
  io.println("\n━━━ MEMORY STATE ━━━")
  let mem = init_memory()
  io.println(
    "  Total VRAM:    " <> int.to_string(mem.total_bytes / 1024 / 1024) <> " MB",
  )
  io.println(
    "  Free VRAM:     " <> int.to_string(mem.free_bytes / 1024 / 1024) <> " MB",
  )

  // Simulate allocations
  let tensor_size = tensor_memory_bytes([1024, 1024], Int8TensorMode)
  io.println(
    "\n  Tensor 1024x1024 INT8: " <> int.to_string(tensor_size / 1024) <> " KB",
  )

  let max_tensors = mem.free_bytes / tensor_size
  io.println("  Tensors that fit:      " <> int.to_string(max_tensors))

  // Batch processing
  io.println("\n━━━ BATCH PROCESSING (BEAM Parallel) ━━━")
  let batch_sizes = [100, 500, 1000]

  list.each(batch_sizes, fn(n) {
    let tensors =
      list.range(1, n)
      |> list.map(fn(_) { tensor.random_uniform([512]) })

    let config = default_config()
    let result = process_batch(tensors, config)

    io.println("  " <> int.to_string(n) <> " tensors x 512d:")
    io.println(
      "    Time:        " <> int.to_string(result.total_time_ms) <> "ms",
    )
    io.println(
      "    Throughput:  "
      <> float_to_string(result.throughput_tps)
      <> " tensors/sec",
    )
    io.println(
      "    Compression: " <> float_to_string(result.compression_ratio) <> "x",
    )
    io.println(
      "    Savings:     " <> float_to_string(result.memory_saved_mb) <> " MB",
    )
    io.println("")
  })

  // Performance estimation
  io.println("━━━ PERFORMANCE ESTIMATION ━━━")

  // Matmul 4096x4096x4096 = 137B FLOPs
  let matmul_flops = 4096.0 *. 4096.0 *. 4096.0 *. 2.0
  let matmul_bytes = 4096.0 *. 4096.0 *. 4.0 *. 3.0
  // A, B, C matrices

  io.println("  Matmul 4096x4096:")

  let est_fp32 =
    estimate_performance(matmul_flops, matmul_bytes, precision_config())
  io.println(
    "    FP32:  "
    <> float_to_string(est_fp32.estimated_time_ms)
    <> "ms, "
    <> bottleneck_str(est_fp32.bottleneck),
  )

  let est_fp16 =
    estimate_performance(
      matmul_flops,
      matmul_bytes /. 2.0,
      Rtx4090Config(..default_config(), quant_mode: Fp16TensorMode),
    )
  io.println(
    "    FP16:  "
    <> float_to_string(est_fp16.estimated_time_ms)
    <> "ms, "
    <> bottleneck_str(est_fp16.bottleneck),
  )

  let est_int8 =
    estimate_performance(matmul_flops, matmul_bytes /. 4.0, default_config())
  io.println(
    "    INT8:  "
    <> float_to_string(est_int8.estimated_time_ms)
    <> "ms, "
    <> bottleneck_str(est_int8.bottleneck),
  )

  // Recommendations
  io.println(
    "\n╔══════════════════════════════════════════════════════════════════╗",
  )
  io.println(
    "║  RECOMMENDATIONS FOR YOUR RTX 4090:                              ║",
  )
  io.println(
    "║                                                                  ║",
  )
  io.println(
    "║  1. Use INT8 Tensor Cores for inference (661 TOPS!)              ║",
  )
  io.println(
    "║  2. Optimal batch size: 128-256 tensors                          ║",
  )
  io.println(
    "║  3. Align memory to 32 bytes (256-bit bus)                       ║",
  )
  io.println(
    "║  4. Tile size 16x16 for Tensor Cores                             ║",
  )
  io.println(
    "║  5. 22GB usable VRAM = ~22M tensors of 1KB                       ║",
  )
  io.println(
    "║                                                                  ║",
  )
  io.println(
    "║  With INT8 compression: 24GB VRAM -> 96GB effective!             ║",
  )
  io.println(
    "╚══════════════════════════════════════════════════════════════════╝",
  )
}

// ============================================================================
// HELPERS
// ============================================================================

fn float_to_string(f: Float) -> String {
  let rounded = int.to_float(float.round(f *. 100.0)) /. 100.0
  float.to_string(rounded)
}

fn bottleneck_str(b: Bottleneck) -> String {
  case b {
    ComputeBound -> "compute-bound"
    MemoryBound -> "memory-bound"
    LatencyBound -> "latency-bound"
  }
}

// FFI
type Pid

@external(erlang, "erlang", "self")
fn erlang_self() -> Pid

@external(erlang, "erlang", "spawn")
fn erlang_spawn(f: fn() -> a) -> Pid

@external(erlang, "viva_tensor_ffi", "send_msg")
fn erlang_send(to: Pid, msg: a) -> a

@external(erlang, "viva_tensor_ffi", "collect_n")
fn collect_n(n: Int) -> List(#(Int, BlackwellTensor))

@external(erlang, "erlang", "monotonic_time")
fn erlang_monotonic_time() -> Int
